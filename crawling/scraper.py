import asyncio
import codecs
import re
import sqlite3
import sys
from datetime import datetime

import aiohttp
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from tqdm.asyncio import tqdm

sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "replace")



# **éš¨æ©Ÿ User-Agent**
def get_headers():
    ua = UserAgent()
    return {"User-Agent": ua.random}


# **ç²å–æœ€å¾Œä¸€é é ç¢¼**
async def get_last_page(session, region, url1, url2):
    try:
        async with session.get(url1 + "1" + f"&region={region}" + url2, headers=get_headers()) as res:
            html = await res.text()
            soup = BeautifulSoup(html, "html.parser")
            elements = soup.find_all('li', attrs={"data-v-779297d8": ""})
            last_number = max([int(el.text.strip()) for el in elements if el.text.strip().isdigit()], default=1)
            return last_number
    except Exception as e:
        print(f"[ERROR] ç²å– {region} çš„æœ€å¾Œä¸€é æ•¸å¤±æ•—: {e}")
        return 1


# **éåŒæ­¥çˆ¬å– post_id**
async def fetch_post_ids(session, region, page, url1, url2):
    try:
        async with session.get(url1 + str(page) + f"&region={region}" + url2, headers=get_headers()) as res:
            soup = BeautifulSoup(await res.text(), "html.parser")
            links = soup.find_all('a', class_="link v-middle")
            return [link['href'].split('/')[-1] for link in links if link.has_attr('href')]
    except Exception as e:
        print(f"[ERROR] çˆ¬å– {region} é ç¢¼ {page} çš„ post_id å¤±æ•—: {e}")
        return []


# **æ‰¹é‡çˆ¬å–æ‰€æœ‰ post_id**
async def crawl_all_post_ids(region, url1, url2):
    async with aiohttp.ClientSession() as session:
        last_page = await get_last_page(session, region, url1, url2)
        tasks = [fetch_post_ids(session, region, page, url1, url2) for page in range(1, last_page + 1)]
        results = await tqdm.gather(*tasks)
        return [post_id for sublist in results for post_id in sublist]


# **çˆ¬å–è©³ç´°è³‡è¨Š**
async def fetch_post_details(session, post_id):
    url = f"https://business.591.com.tw/rent/{post_id}"
    try:
        async with session.get(url, headers=get_headers()) as res:
            soup = BeautifulSoup(await res.text(), "html.parser")

            # **æŠ“å–åŸºæœ¬è³‡è¨Š**
            span_tags = soup.find_all('span', attrs={'data-v-588d0396': ''})
            extracted_texts = [tag.text.strip() for tag in span_tags]

            patterns = {
                'owner': re.compile(r'(å±‹ä¸»|ä»£ç†äºº|ä»²ä»‹): .+'),
                'price': re.compile(r'\d{1,3}(,\d{3})*å…ƒ/æœˆ'),
                'phone': re.compile(r'\d{4}-\d{3}-\d{3}|\d{2}-\d{8}|\d{2}-\d{7}')
            }

            results = {
                'id': post_id, 'owner': None, 'price': None, 'phone': None,
                'location': None, 'place': None, 'squaremeter': None, 'floor': None, 'type': None,
                'latitude': None, 'longitude': None
            }

            for text in extracted_texts:
                for key, pattern in patterns.items():
                    if pattern.match(text):
                        results[key] = text

            # **æŠ“å–åœ°å€**
            address_tag = soup.find('div', class_="address")
            if address_tag:
                results['location'] = address_tag.text.strip()

            # **æŠ“å–å€åŸŸ**
            place_tag = soup.find('div', class_="place")
            if place_tag:
                results['place'] = place_tag.text.strip()

            # **æŠ“å–åªæ•¸ã€æ¨“å±¤ã€é¡å‹**
            info_tags = soup.find_all('div', class_="info")
            info_values = [tag.text.strip() for tag in info_tags]

            if len(info_values) >= 3:
                results['squaremeter'], results['floor'], results['type'] = info_values[:3]

            # **æŠ“å–ç¶“ç·¯åº¦**
            script_tags = soup.find_all("script")
            for script in script_tags:
                match = re.search(r'latitude\s*:\s*([\d.]+),\s*longitude\s*:\s*([\d.]+)', script.text)
                if match:
                    results['latitude'], results['longitude'] = match.groups()
                    break

            return results
    except Exception as e:
        print(f"[ERROR] çˆ¬å– post_id {post_id} è©³ç´°è³‡è¨Šå¤±æ•—: {e}")
        return results


# **æ‰¹é‡çˆ¬å–æ‰€æœ‰ post è©³ç´°è³‡è¨Š**
async def crawl_all_post_details(post_ids):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_post_details(session, post_id) for post_id in post_ids]
        results = await tqdm.gather(*tasks)
        return results


# **å„²å­˜åˆ° SQLite**
def save_to_db(data_list):
    conn = sqlite3.connect("scraped_data.db")
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS scraped_data (
            id TEXT PRIMARY KEY,
            owner TEXT,
            price TEXT,
            location TEXT,
            place TEXT,
            phone TEXT,
            squaremeter TEXT,
            floor TEXT,
            type TEXT,
            latitude TEXT,
            longitude TEXT,
            timestamp TEXT
        )
    """)

    for data in data_list:
        cursor.execute("""
            INSERT OR REPLACE INTO scraped_data 
            (id, owner, price, location, place, phone, squaremeter, floor, type, latitude, longitude, timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            data['id'], data['owner'], data['price'], data['location'], data['place'],
            data['phone'], data['squaremeter'], data['floor'], data['type'],
            data['latitude'], data['longitude'], datetime.now()
        ))

    conn.commit()
    conn.close()


# **ä¸»ç¨‹å¼**
async def main():
    try:
        print("é–‹å§‹åŸ·è¡Œä¸»ç¨‹å¼...")
        
        area_dict = {
            "å°åŒ—å¸‚": (1, "https://business.591.com.tw/list?type=1&kind=6&region=1&page=", "")
        }


        all_results = []
        for region_name, (area_list, url_all, url_condition) in area_dict.items():
            for region in area_list:
                print(f"\nğŸš€ æ­£åœ¨çˆ¬å– {region_name} (å€åŸŸID: {region}) çš„è³‡æ–™...")
                post_ids = await crawl_all_post_ids(region, url_all, url_condition)
                print(f"âœ… ç²å– {len(post_ids)} ç­† post_idï¼Œé–‹å§‹çˆ¬å–è©³ç´°è³‡è¨Š...")

                post_details = await crawl_all_post_details(post_ids)
                all_results.extend(post_details)

        save_to_db(all_results)
        print("âœ… çˆ¬å–å®Œæˆï¼Œæ•¸æ“šå·²å­˜å…¥ SQLiteã€‚")
        print(all_results)

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")


if __name__ == "__main__":
    asyncio.run(main())
